{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling SARS-CoV-2 variants from a metagenomic profiling sample\n",
    "\n",
    "In this assignment, we are going to call SARS-CoV-2 variants from a metagenomic profiling sample.\n",
    "\n",
    "To call variants, we need to compare it to a wild-type reference sequence, which is usually the reference genome, but where does these reference genome came from?\n",
    "\n",
    "There are several ways of doing it. The reference genome for SARS-CoV-2 was generated as follows ([reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7094943/)):\n",
    "\n",
    "1. Patient with pneumonia of unknown origin were examined, and bronchioaveolar lavage fluid was collected (from washing the respiratory tract with saline).\n",
    "\n",
    "2. Assuming the RNA extraction, RT-PCR, and library construction was performed followed by deep RNA-Seq (56 million reads).\n",
    "\n",
    "3. Assuming that when we sequence deep enough, every read will have at least one other read that partially overlaps with it, so in theory, we can assemble the whole transcriptome by carefully concatenating reads according to the overlap (For details, check: de novo assembly).\n",
    "\n",
    "\n",
    "The data we are analyzing is also RNA-seq but is a pool of nasal swab samples, so it is expected to sample different variants that are endemic in the region where samples are collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: If you are interested in how we retrieve that data from GEO datasets\n",
    "\n",
    "The data we are analyzing is public and retrieved from https://www.ncbi.nlm.nih.gov/sra/SRR11801823. To retrieve sequence file from GEO dataset, `sra-tools` provides easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd data/raw_seq\n",
    "\n",
    "# fastq-dump downloads data in fastq format\n",
    "## --split-3 forces sra-tools to check if the data is paired-ended or not.\n",
    "## The 3 comes from the fact that some sequencing experiments splits read in 3 proportions.\n",
    "## For example, single-cell RNA-seq using 10X Genomics Chromium platform are sequenced in a paired-ended mode \n",
    "## and gives read 1 and read 2, but there's an additional index read.\n",
    "## `--split-3` makes sure these reads are downloaded separatedly.\n",
    "fastq-dump --split-3 SRR11801823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check the adapter database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Illumina Single End Apapter 1\t\t\t\t\t\n",
      "ACACTCTTTCCCTACACGACGCTGTTCCATCT\n",
      ">Illumina Single End Apapter 2\t\t\t\t\t\n",
      "CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT\n",
      ">Illumina Single End PCR Primer 1\t\t\t\t\n",
      "AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT\n",
      ">Illumina Single End PCR Primer 2\t\t\t\t\n",
      "CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT\n",
      ">Illumina Single End Sequencing Primer\t\t\t\n",
      "ACACTCTTTCCCTACACGACGCTCTTCCGATCT\n"
     ]
    }
   ],
   "source": [
    "head Sequencing_adaptors.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasta files is a simple format in which there are header lines (starting with >) containing metadata/information about a sequence, and the sequence in the next line. If you were to add a new entry into the file, just open the file with a text editor, and add the following at the bottom of the file:\n",
    "\n",
    "```\n",
    ">Name of my sequence\n",
    "[Sequence]\n",
    "```\n",
    "\n",
    "Adapter files that trimming tools (e.g., Trimmomatic, Cutadapt...) use are usually fasta files containing the known adapters. The tools will use the adapter sequence and the threshold provided to find and remove the seqeunces.\n",
    "\n",
    "Note that it is not advisable to use a comprehensive adapter database. Instead, we should always know what adapters (corresponding to the library generation kit) we are using, and this information should be reported when we publish the study and deposit the data in a public database like GEO Profile.\n",
    "\n",
    "Therefore, if we know we are working with an experiment using TruSeqv3 kit, we should only trim with adpater sequence corresponding to TruSeqv3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Illumina Paried End Sequencing Primer 1\t\t\t\n",
      "ACACTCTTTCCCTACACGACGCTCTTCCGATCT\n"
     ]
    }
   ],
   "source": [
    "# -A asks grep to also print the line *A*fter the match is found\n",
    "grep \"Illumina.*Sequencing Primer 1\" Sequencing_adaptors.fasta -A 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform FastQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started analysis of SRR11801823_1.fastq\n",
      "Approx 5% complete for SRR11801823_1.fastq\n",
      "Approx 10% complete for SRR11801823_1.fastq\n",
      "Approx 15% complete for SRR11801823_1.fastq\n",
      "Approx 20% complete for SRR11801823_1.fastq\n",
      "Approx 25% complete for SRR11801823_1.fastq\n",
      "Approx 30% complete for SRR11801823_1.fastq\n",
      "Approx 35% complete for SRR11801823_1.fastq\n",
      "Approx 40% complete for SRR11801823_1.fastq\n",
      "Approx 45% complete for SRR11801823_1.fastq\n",
      "Approx 50% complete for SRR11801823_1.fastq\n",
      "Approx 55% complete for SRR11801823_1.fastq\n",
      "Approx 60% complete for SRR11801823_1.fastq\n",
      "Approx 65% complete for SRR11801823_1.fastq\n",
      "Approx 70% complete for SRR11801823_1.fastq\n",
      "Approx 75% complete for SRR11801823_1.fastq\n",
      "Approx 80% complete for SRR11801823_1.fastq\n",
      "Approx 85% complete for SRR11801823_1.fastq\n",
      "Approx 90% complete for SRR11801823_1.fastq\n",
      "Approx 95% complete for SRR11801823_1.fastq\n",
      "Analysis complete for SRR11801823_1.fastq\n",
      "Started analysis of SRR11801823_2.fastq\n",
      "Approx 5% complete for SRR11801823_2.fastq\n",
      "Approx 10% complete for SRR11801823_2.fastq\n",
      "Approx 15% complete for SRR11801823_2.fastq\n",
      "Approx 20% complete for SRR11801823_2.fastq\n",
      "Approx 25% complete for SRR11801823_2.fastq\n",
      "Approx 30% complete for SRR11801823_2.fastq\n",
      "Approx 35% complete for SRR11801823_2.fastq\n",
      "Approx 40% complete for SRR11801823_2.fastq\n",
      "Approx 45% complete for SRR11801823_2.fastq\n",
      "Approx 50% complete for SRR11801823_2.fastq\n",
      "Approx 55% complete for SRR11801823_2.fastq\n",
      "Approx 60% complete for SRR11801823_2.fastq\n",
      "Approx 65% complete for SRR11801823_2.fastq\n",
      "Approx 70% complete for SRR11801823_2.fastq\n",
      "Approx 75% complete for SRR11801823_2.fastq\n",
      "Approx 80% complete for SRR11801823_2.fastq\n",
      "Approx 85% complete for SRR11801823_2.fastq\n",
      "Approx 90% complete for SRR11801823_2.fastq\n",
      "Approx 95% complete for SRR11801823_2.fastq\n",
      "Analysis complete for SRR11801823_2.fastq\n"
     ]
    }
   ],
   "source": [
    "module purge\n",
    "module load fastqc/0.11.9\n",
    "\n",
    "# Create a folder to store the output\n",
    "\n",
    "mkdir fastqc\n",
    "fastqc ./data/raw_seq/SRR11801823_1.fastq \\\n",
    "       ./data/raw_seq/SRR11801823_2.fastq \\\n",
    "       -o fastqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Trimming adapters and low-quality bases\n",
    "\n",
    "We do see adapter enrichment in FastQC report. It is worth noting that though FastQC detects most commonly used adapters, the adpater from your kit might not be in their database.\n",
    "\n",
    "The data we are analyzing is constructed with an NEB kit, so we will add adapter information for NEB to our adapter database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">NEB Universal adapter 1\n",
      "AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\n",
      ">NEB Universal adapter 2\n",
      "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT"
     ]
    }
   ],
   "source": [
    "# Add NEB adapter data\n",
    "cat NEB_adapt.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat NEB_adapt.fa >> Sequencing_adaptors.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘int’: File exists\n",
      "TrimmomaticPE: Started with arguments:\n",
      " -phred33 data/raw_seq/SRR11801823_1.fastq data/raw_seq/SRR11801823_2.fastq int/read_1_trimmed_n.fq int/read_1_unpair_trimmed_n.fq int/read_2_trimmed_n.fq int/read_2_unpair_trimmed_n.fq ILLUMINACLIP:Sequencing_adaptors.fasta:2:30:10:2:keepBothReads TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:100\n",
      "Multiple cores found: Using 4 threads\n",
      "Using Long Clipping Sequence: 'CAAGCAGAAGACGGCATACGAGATTGCCGAGTGACTGGAGTTCCTTGGCACCCGAGAATTCCA'\n",
      "Using Long Clipping Sequence: 'CTTACTCCTTGGAGGCCATG>NEB Universal adapter 1AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC'\n",
      "Using Long Clipping Sequence: 'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT'\n",
      "Using Long Clipping Sequence: 'GATCGGAAGAGCACACGTCTGAACTCCAGTCACCTTGTAATCTCGTATGCCGTCTTCTGCTTG'\n",
      "Using Long Clipping Sequence: 'AATGATACGGCGACCACCGAGATCTACACGTTCAGAGTTCTACAGTCCGA'\n",
      "ILLUMINACLIP: Using 0 prefix pairs, 5 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n",
      "Input Read Pairs: 512892 Both Surviving: 242426 (47.27%) Forward Only Surviving: 3797 (0.74%) Reverse Only Surviving: 155177 (30.26%) Dropped: 111492 (21.74%)\n",
      "TrimmomaticPE: Completed successfully\n"
     ]
    }
   ],
   "source": [
    "module load trimmomatic/0.39\n",
    "\n",
    "mkdir int\n",
    "java -jar $TRIMMOMATIC_JAR PE -phred33 \\\n",
    "data/raw_seq/SRR11801823_1.fastq data/raw_seq/SRR11801823_2.fastq \\\n",
    "int/read_1_trimmed_n.fq int/read_1_unpair_trimmed_n.fq \\\n",
    "int/read_2_trimmed_n.fq int/read_2_unpair_trimmed_n.fq \\\n",
    "ILLUMINACLIP:Sequencing_adaptors.fasta:2:30:10:2:keepBothReads TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started analysis of read_1_trimmed_n.fq\n",
      "Approx 5% complete for read_1_trimmed_n.fq\n",
      "Approx 10% complete for read_1_trimmed_n.fq\n",
      "Approx 15% complete for read_1_trimmed_n.fq\n",
      "Approx 20% complete for read_1_trimmed_n.fq\n",
      "Approx 25% complete for read_1_trimmed_n.fq\n",
      "Approx 30% complete for read_1_trimmed_n.fq\n",
      "Approx 35% complete for read_1_trimmed_n.fq\n",
      "Approx 40% complete for read_1_trimmed_n.fq\n",
      "Approx 45% complete for read_1_trimmed_n.fq\n",
      "Approx 50% complete for read_1_trimmed_n.fq\n",
      "Approx 55% complete for read_1_trimmed_n.fq\n",
      "Approx 60% complete for read_1_trimmed_n.fq\n",
      "Approx 65% complete for read_1_trimmed_n.fq\n",
      "Approx 70% complete for read_1_trimmed_n.fq\n",
      "Approx 75% complete for read_1_trimmed_n.fq\n",
      "Approx 80% complete for read_1_trimmed_n.fq\n",
      "Approx 85% complete for read_1_trimmed_n.fq\n",
      "Approx 90% complete for read_1_trimmed_n.fq\n",
      "Approx 95% complete for read_1_trimmed_n.fq\n",
      "Analysis complete for read_1_trimmed_n.fq\n",
      "Started analysis of read_2_trimmed_n.fq\n",
      "Approx 5% complete for read_2_trimmed_n.fq\n",
      "Approx 10% complete for read_2_trimmed_n.fq\n",
      "Approx 15% complete for read_2_trimmed_n.fq\n",
      "Approx 20% complete for read_2_trimmed_n.fq\n",
      "Approx 25% complete for read_2_trimmed_n.fq\n",
      "Approx 30% complete for read_2_trimmed_n.fq\n",
      "Approx 35% complete for read_2_trimmed_n.fq\n",
      "Approx 40% complete for read_2_trimmed_n.fq\n",
      "Approx 45% complete for read_2_trimmed_n.fq\n",
      "Approx 50% complete for read_2_trimmed_n.fq\n",
      "Approx 55% complete for read_2_trimmed_n.fq\n",
      "Approx 60% complete for read_2_trimmed_n.fq\n",
      "Approx 65% complete for read_2_trimmed_n.fq\n",
      "Approx 70% complete for read_2_trimmed_n.fq\n",
      "Approx 75% complete for read_2_trimmed_n.fq\n",
      "Approx 80% complete for read_2_trimmed_n.fq\n",
      "Approx 85% complete for read_2_trimmed_n.fq\n",
      "Approx 90% complete for read_2_trimmed_n.fq\n",
      "Approx 95% complete for read_2_trimmed_n.fq\n",
      "Analysis complete for read_2_trimmed_n.fq\n"
     ]
    }
   ],
   "source": [
    "# And we run fastqc again\n",
    "fastqc int/read_1_trimmed_n.fq \\\n",
    "       int/read_2_trimmed_n.fq \\\n",
    "       -o fastqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the fastqc report that before trimming, the length of every read is 151 bp, while after trimming, the range becomes 100 - 151, and adapter enrichment at the 3' end is gone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Align to the genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bwa_index] Pack FASTA... 0.00 sec\n",
      "[bwa_index] Construct BWT for the packed sequence...\n",
      "[bwa_index] 0.00 seconds elapse.\n",
      "[bwa_index] Update BWT... 0.00 sec\n",
      "[bwa_index] Pack forward-only FASTA... 0.00 sec\n",
      "[bwa_index] Construct SA from BWT and Occ... 0.00 sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa index NC_045512.2.fa\n",
      "[main] Real time: 0.406 sec; CPU: 0.016 sec\n"
     ]
    }
   ],
   "source": [
    "module load bwa/intel/0.7.17\n",
    "\n",
    "# For the first time running it, we'll need to index the genome\n",
    "cd data/genome\n",
    "bwa index NC_045512.2.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[M::process] read 66472 sequences (10000123 bp)...\n",
      "[M::process] read 66440 sequences (10000141 bp)...\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (49, 33004, 68, 46)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (76, 87, 135)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 253)\n",
      "[M::mem_pestat] mean and std.dev: (94.40, 33.66)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 312)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (148, 197, 261)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 487)\n",
      "[M::mem_pestat] mean and std.dev: (214.44, 79.91)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 600)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (132, 277, 957)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 2607)\n",
      "[M::mem_pestat] mean and std.dev: (467.08, 534.94)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 3432)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (75, 96, 160)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 330)\n",
      "[M::mem_pestat] mean and std.dev: (93.69, 40.61)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 415)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 66472 reads in 2.327 CPU sec, 2.278 real sec\n",
      "[M::process] read 66442 sequences (10000001 bp)...\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (40, 33003, 56, 52)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (73, 88, 144)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 286)\n",
      "[M::mem_pestat] mean and std.dev: (83.19, 26.36)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 357)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (149, 197, 260)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 482)\n",
      "[M::mem_pestat] mean and std.dev: (213.30, 78.03)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 593)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (144, 346, 971)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 2625)\n",
      "[M::mem_pestat] mean and std.dev: (444.58, 506.32)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 3452)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (69, 83, 130)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 252)\n",
      "[M::mem_pestat] mean and std.dev: (88.04, 35.27)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 313)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 66440 reads in 2.315 CPU sec, 2.239 real sec\n",
      "[M::process] read 66434 sequences (10000054 bp)...\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (42, 33028, 58, 46)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (62, 93, 125)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 251)\n",
      "[M::mem_pestat] mean and std.dev: (78.14, 32.00)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 314)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (149, 198, 263)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 491)\n",
      "[M::mem_pestat] mean and std.dev: (215.33, 80.25)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 605)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (112, 557, 1719)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 4933)\n",
      "[M::mem_pestat] mean and std.dev: (803.08, 1083.03)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 6540)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (81, 101, 176)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 366)\n",
      "[M::mem_pestat] mean and std.dev: (99.41, 38.53)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 461)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 66442 reads in 2.337 CPU sec, 2.270 real sec\n",
      "[M::process] read 66432 sequences (10000127 bp)...\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (39, 32994, 79, 53)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (86, 104, 160)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 308)\n",
      "[M::mem_pestat] mean and std.dev: (109.48, 43.85)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 382)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (149, 198, 263)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 491)\n",
      "[M::mem_pestat] mean and std.dev: (215.55, 80.81)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 605)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (97, 340, 909)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 2533)\n",
      "[M::mem_pestat] mean and std.dev: (466.86, 496.38)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 3345)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (77, 96, 123)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 215)\n",
      "[M::mem_pestat] mean and std.dev: (90.64, 31.84)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 261)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 66434 reads in 2.388 CPU sec, 2.312 real sec\n",
      "[M::process] read 66458 sequences (10000176 bp)...\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (61, 32941, 74, 52)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (81, 91, 134)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 240)\n",
      "[M::mem_pestat] mean and std.dev: (91.55, 30.69)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 293)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (149, 198, 267)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 503)\n",
      "[M::mem_pestat] mean and std.dev: (216.71, 82.35)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 621)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (161, 315, 922)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 2444)\n",
      "[M::mem_pestat] mean and std.dev: (458.79, 501.76)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 3205)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (78, 91, 156)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 312)\n",
      "[M::mem_pestat] mean and std.dev: (98.53, 48.05)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 390)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 66432 reads in 2.403 CPU sec, 2.329 real sec\n",
      "[M::process] read 66490 sequences (10000254 bp)...\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (62, 32946, 77, 63)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (75, 92, 134)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 252)\n",
      "[M::mem_pestat] mean and std.dev: (85.94, 25.98)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 311)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (149, 198, 265)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 497)\n",
      "[M::mem_pestat] mean and std.dev: (215.97, 82.17)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 613)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (115, 558, 1294)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 3652)\n",
      "[M::mem_pestat] mean and std.dev: (780.25, 826.30)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 4831)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (77, 88, 120)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 206)\n",
      "[M::mem_pestat] mean and std.dev: (88.59, 29.56)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 249)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 66458 reads in 2.422 CPU sec, 2.348 real sec\n",
      "[M::process] read 19684 sequences (2960009 bp)...\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (44, 32982, 85, 62)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (85, 99, 224)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 502)\n",
      "[M::mem_pestat] mean and std.dev: (98.47, 40.11)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 641)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (148, 198, 264)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 496)\n",
      "[M::mem_pestat] mean and std.dev: (215.47, 81.69)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 612)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (161, 485, 1189)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 3245)\n",
      "[M::mem_pestat] mean and std.dev: (674.95, 625.47)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 4273)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (80, 96, 141)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 263)\n",
      "[M::mem_pestat] mean and std.dev: (99.25, 36.08)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 324)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 66490 reads in 2.375 CPU sec, 2.332 real sec\n",
      "[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (14, 9766, 15, 27)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (74, 91, 94)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (34, 134)\n",
      "[M::mem_pestat] mean and std.dev: (84.45, 17.25)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (14, 154)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (148, 198, 261)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 487)\n",
      "[M::mem_pestat] mean and std.dev: (213.60, 79.80)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 600)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (99, 153, 341)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 825)\n",
      "[M::mem_pestat] mean and std.dev: (162.92, 124.40)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 1067)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (76, 92, 110)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (8, 178)\n",
      "[M::mem_pestat] mean and std.dev: (84.26, 21.22)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 212)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_process_seqs] Processed 19684 reads in 0.737 CPU sec, 0.699 real sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem data/genome/NC_045512.2.fa int/read_1_trimmed_n.fq int/read_2_trimmed_n.fq\n",
      "[main] Real time: 17.086 sec; CPU: 17.390 sec\n"
     ]
    }
   ],
   "source": [
    "cd ../..\n",
    "mkdir result\n",
    "# Align the reads\n",
    "bwa mem data/genome/NC_045512.2.fa \\\n",
    "        int/read_1_trimmed_n.fq \\\n",
    "        int/read_2_trimmed_n.fq > result/aligned_reads.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert to BAM and sort by coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2021-03-24 14:06:29\tSortSam\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    SortSam -INPUT result/aligned_reads.sam -OUTPUT result/sorted_reads.bam -SORT_ORDER coordinate\n",
      "**********\n",
      "\n",
      "\n",
      "14:06:29.395 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/picard/2.23.8/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Mar 24 14:06:29 EDT 2021] SortSam INPUT=result/aligned_reads.sam OUTPUT=result/sorted_reads.bam SORT_ORDER=coordinate    VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Wed Mar 24 14:06:29 EDT 2021] Executing as ycc520@cs053.nyu.cluster on Linux 4.18.0-193.28.1.el8_2.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_271-b09; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.23.8\n",
      "INFO\t2021-03-24 14:06:29\tSortSam\tSeen many non-increasing record positions. Printing Read-names as well.\n",
      "INFO\t2021-03-24 14:06:36\tSortSam\tFinished reading inputs, merging and writing to output now.\n",
      "[Wed Mar 24 14:06:38 EDT 2021] picard.sam.SortSam done. Elapsed time: 0.16 minutes.\n",
      "Runtime.totalMemory()=1224736768\n"
     ]
    }
   ],
   "source": [
    "module load picard/2.23.8\n",
    "\n",
    "java -jar $PICARD_JAR SortSam \\\n",
    "          INPUT=result/aligned_reads.sam \\\n",
    "          OUTPUT=result/sorted_reads.bam \\\n",
    "          SORT_ORDER=coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add read group information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2021-03-24 14:08:04\tAddOrReplaceReadGroups\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    AddOrReplaceReadGroups -I result/sorted_reads.bam -O result/sorted_reads_rg.bam -RGID SRR11801823 -RGLB SRR11801823 -RGPL ILLUMINA -RGPM iSEQ -RGPU SRR11801823 -RGSM SRR11801823\n",
      "**********\n",
      "\n",
      "\n",
      "14:08:04.556 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/picard/2.23.8/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Mar 24 14:08:04 EDT 2021] AddOrReplaceReadGroups INPUT=result/sorted_reads.bam OUTPUT=result/sorted_reads_rg.bam RGID=SRR11801823 RGLB=SRR11801823 RGPL=ILLUMINA RGPU=SRR11801823 RGSM=SRR11801823 RGPM=iSEQ    VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Wed Mar 24 14:08:04 EDT 2021] Executing as ycc520@cs053.nyu.cluster on Linux 4.18.0-193.28.1.el8_2.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_271-b09; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.23.8\n",
      "INFO\t2021-03-24 14:08:04\tAddOrReplaceReadGroups\tCreated read-group ID=SRR11801823 PL=ILLUMINA LB=SRR11801823 SM=SRR11801823\n",
      "\n",
      "[Wed Mar 24 14:08:08 EDT 2021] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 0.06 minutes.\n",
      "Runtime.totalMemory()=458227712\n"
     ]
    }
   ],
   "source": [
    "## Add read group to the bam file\n",
    "## The platform and model information, while not relevant for this analysis,\n",
    "## can be found at the information page on GEO dataset\n",
    "java -jar $PICARD_JAR AddOrReplaceReadGroups \\\n",
    "          I=result/sorted_reads.bam \\\n",
    "          O=result/sorted_reads_rg.bam \\\n",
    "          RGID=SRR11801823 \\\n",
    "          RGLB=SRR11801823 \\\n",
    "          RGPL=ILLUMINA \\\n",
    "          RGPM=iSEQ \\\n",
    "          RGPU=SRR11801823 \\\n",
    "          RGSM=SRR11801823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Mark duplicated reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2021-03-24 14:08:39\tMarkDuplicates\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    MarkDuplicates -INPUT result/sorted_reads_rg.bam -OUTPUT result/dedup_reads.bam -METRICS_FILE metrics.txt\n",
      "**********\n",
      "\n",
      "\n",
      "14:08:39.955 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/picard/2.23.8/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Mar 24 14:08:39 EDT 2021] MarkDuplicates INPUT=[result/sorted_reads_rg.bam] OUTPUT=result/dedup_reads.bam METRICS_FILE=metrics.txt    MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true DUPLEX_UMI=false ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Wed Mar 24 14:08:39 EDT 2021] Executing as ycc520@cs053.nyu.cluster on Linux 4.18.0-193.28.1.el8_2.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_271-b09; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.23.8\n",
      "INFO\t2021-03-24 14:08:40\tMarkDuplicates\tStart of doWork freeMemory: 149890152; totalMemory: 160956416; maxMemory: 2386558976\n",
      "INFO\t2021-03-24 14:08:40\tMarkDuplicates\tReading input file and constructing read end information.\n",
      "INFO\t2021-03-24 14:08:40\tMarkDuplicates\tWill retain up to 8646952 data points before spilling to disk.\n",
      "WARNING\t2021-03-24 14:08:40\tAbstractOpticalDuplicateFinderCommandLineProgram\tA field field parsed out of a read name was expected to contain an integer and did not. Read name: SRR11801823.111780. Cause: String 'SRR11801823.111780' did not start with a parsable number.\n",
      "INFO\t2021-03-24 14:08:42\tMarkDuplicates\tRead 503404 records. 0 pairs never matched.\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tAfter buildSortedReadEndLists freeMemory: 466943176; totalMemory: 600309760; maxMemory: 2386558976\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tWill retain up to 74579968 duplicate indices before spilling to disk.\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tTraversing read pair information and detecting duplicates.\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tTraversing fragment information and detecting duplicates.\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tSorting list of duplicate records.\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tAfter generateDuplicateIndexes freeMemory: 588833224; totalMemory: 1197473792; maxMemory: 2386558976\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tMarking 209381 records as duplicates.\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tFound 0 optical duplicate clusters.\n",
      "INFO\t2021-03-24 14:08:43\tMarkDuplicates\tReads are assumed to be ordered by: coordinate\n",
      "INFO\t2021-03-24 14:08:47\tMarkDuplicates\tWriting complete. Closing input iterator.\n",
      "INFO\t2021-03-24 14:08:47\tMarkDuplicates\tDuplicate Index cleanup.\n",
      "INFO\t2021-03-24 14:08:47\tMarkDuplicates\tGetting Memory Stats.\n",
      "INFO\t2021-03-24 14:08:47\tMarkDuplicates\tBefore output close freeMemory: 1286377080; totalMemory: 1300758528; maxMemory: 2386558976\n",
      "INFO\t2021-03-24 14:08:47\tMarkDuplicates\tClosed outputs. Getting more Memory Stats.\n",
      "INFO\t2021-03-24 14:08:47\tMarkDuplicates\tAfter output close freeMemory: 1286901368; totalMemory: 1301282816; maxMemory: 2386558976\n",
      "[Wed Mar 24 14:08:47 EDT 2021] picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 0.13 minutes.\n",
      "Runtime.totalMemory()=1301282816\n"
     ]
    }
   ],
   "source": [
    "java -jar $PICARD_JAR MarkDuplicates \\\n",
    "          INPUT=result/sorted_reads_rg.bam \\\n",
    "          OUTPUT=result/dedup_reads.bam \\\n",
    "          METRICS_FILE=metrics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MarkDuplicate mode of picard utilizes several information to find possible PCR duplicates: It examines the 5' end of the read, and also try to infer duplicates generated in clutser formation in Illumina sequencing.\n",
    "\n",
    "The coordinate of a read in the sequencing lane will be stored in the read names in resulting fastq files, but GEO dataset (or `sra-tools`) does not preserve this information, as a result, when picard tries to learn coordinates from read names, it struggles and produces warnings like:\n",
    "\n",
    "```\n",
    "WARNING\t2021-03-24 14:08:40\tAbstractOpticalDuplicateFinderCommandLineProgram\tA field field parsed out of a read name was expected to contain an integer and did not. Read name: SRR11801823.111780. Cause: String 'SRR11801823.111780' did not start with a parsable number.\n",
    "```\n",
    "\n",
    "It is safe to ingore the warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prepare reference dictionary, fasta index, and bam index\n",
    "\n",
    "To make sure the aligned reads (BAM file) are aligned to the same reference genome you are using to call variants, the reference genome has to be made a dictionary.\n",
    "\n",
    "Under the table, when fastq files are aligned, the resulting SAM/BAM files contain the version of reference genome in its header.\n",
    "\n",
    "Dictionary files of a reference genome is just extracting version information and save it in SAM header format for the ease of comparison.\n",
    "\n",
    "Indexing the reference genome fasta and the BAM file is to facilitate quick access to them. Indexing is conceptually like providing a table of content.\n",
    "\n",
    "If you don't index a sorted BAM file, when you need to find a read coming from chr17, you'll need to scroll down from chr1, because you don't know exactly how many lines you have to skip to get to chr17. After indexing, information like \"chr17 starts from the kth line\" are stored in the index file, and now other tools can access the fasta/BAM file with a higher efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2021-03-24 14:17:00\tCreateSequenceDictionary\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    CreateSequenceDictionary -R data/genome/NC_045512.2.fa -O data/genome/NC_045512.2.dict\n",
      "**********\n",
      "\n",
      "\n",
      "14:17:00.231 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/picard/2.23.8/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Mar 24 14:17:00 EDT 2021] CreateSequenceDictionary OUTPUT=data/genome/NC_045512.2.dict REFERENCE=data/genome/NC_045512.2.fa    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Wed Mar 24 14:17:00 EDT 2021] Executing as ycc520@cs053.nyu.cluster on Linux 4.18.0-193.28.1.el8_2.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_271-b09; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.23.8\n",
      "[Wed Mar 24 14:17:00 EDT 2021] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.00 minutes.\n",
      "Runtime.totalMemory()=160956416\n"
     ]
    }
   ],
   "source": [
    "java -jar $PICARD_JAR CreateSequenceDictionary \\\n",
    "          R=data/genome/NC_045512.2.fa \\\n",
    "          O=data/genome/NC_045512.2.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@HD\tVN:1.6\n",
      "@SQ\tSN:NC_045512.2\tLN:29903\tM5:105c82802b67521950854a851fc6eefd\tUR:file:/scratch/ycc520/ag_recitation/asm_03_key/data/genome/NC_045512.2.fa\n"
     ]
    }
   ],
   "source": [
    "# A dictionary is just a SAM header\n",
    "cat data/genome/NC_045512.2.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing with samtools\n",
    "module load samtools/intel/1.11\n",
    "\n",
    "samtools faidx data/genome/NC_045512.2.fa\n",
    "samtools index result/dedup_reads.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Call haplotypes with GATK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:19:03.177 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "Mar 24, 2021 2:19:03 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine\n",
      "INFO: Failed to detect whether we are running on Google Compute Engine.\n",
      "14:19:03.304 INFO  HaplotypeCaller - ------------------------------------------------------------\n",
      "14:19:03.305 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.9.0\n",
      "14:19:03.305 INFO  HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "14:19:03.305 INFO  HaplotypeCaller - Executing as ycc520@cs053.nyu.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64\n",
      "14:19:03.305 INFO  HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_271-b09\n",
      "14:19:03.305 INFO  HaplotypeCaller - Start Date/Time: March 24, 2021 2:19:03 PM EDT\n",
      "14:19:03.305 INFO  HaplotypeCaller - ------------------------------------------------------------\n",
      "14:19:03.305 INFO  HaplotypeCaller - ------------------------------------------------------------\n",
      "14:19:03.305 INFO  HaplotypeCaller - HTSJDK Version: 2.23.0\n",
      "14:19:03.305 INFO  HaplotypeCaller - Picard Version: 2.23.3\n",
      "14:19:03.305 INFO  HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "14:19:03.305 INFO  HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "14:19:03.305 INFO  HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n",
      "14:19:03.306 INFO  HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "14:19:03.306 INFO  HaplotypeCaller - Deflater: IntelDeflater\n",
      "14:19:03.306 INFO  HaplotypeCaller - Inflater: IntelInflater\n",
      "14:19:03.306 INFO  HaplotypeCaller - GCS max retries/reopens: 20\n",
      "14:19:03.306 INFO  HaplotypeCaller - Requester pays: disabled\n",
      "14:19:03.306 INFO  HaplotypeCaller - Initializing engine\n",
      "14:19:03.576 INFO  HaplotypeCaller - Done initializing engine\n",
      "14:19:03.581 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output\n",
      "14:19:03.592 INFO  NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_utils.so\n",
      "14:19:03.598 INFO  NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so\n",
      "14:19:03.626 INFO  IntelPairHmm - Using CPU-supported AVX-512 instructions\n",
      "14:19:03.627 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM\n",
      "14:19:03.627 INFO  IntelPairHmm - Available threads: 4\n",
      "14:19:03.627 INFO  IntelPairHmm - Requested threads: 4\n",
      "14:19:03.627 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation\n",
      "14:19:03.644 INFO  ProgressMeter - Starting traversal\n",
      "14:19:03.644 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute\n",
      "14:19:13.045 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes\n",
      "14:19:20.014 INFO  ProgressMeter -     NC_045512.2:1201              0.3                    10             36.7\n",
      "14:19:34.332 INFO  ProgressMeter -    NC_045512.2:11729              0.5                    60            117.3\n",
      "14:19:45.312 INFO  ProgressMeter -    NC_045512.2:20488              0.7                   100            144.0\n",
      "14:19:55.362 INFO  ProgressMeter -    NC_045512.2:26519              0.9                   130            150.8\n",
      "14:20:09.076 INFO  ProgressMeter -    NC_045512.2:28928              1.1                   140            128.4\n",
      "14:20:21.153 INFO  HaplotypeCaller - 3 read(s) filtered by: MappingQualityReadFilter \n",
      "0 read(s) filtered by: MappingQualityAvailableReadFilter \n",
      "0 read(s) filtered by: MappedReadFilter \n",
      "0 read(s) filtered by: NotSecondaryAlignmentReadFilter \n",
      "209381 read(s) filtered by: NotDuplicateReadFilter \n",
      "0 read(s) filtered by: PassesVendorQualityCheckReadFilter \n",
      "0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter \n",
      "0 read(s) filtered by: GoodCigarReadFilter \n",
      "0 read(s) filtered by: WellformedReadFilter \n",
      "209384 total reads filtered\n",
      "14:20:21.154 INFO  ProgressMeter -    NC_045512.2:28928              1.3                   145            112.2\n",
      "14:20:21.154 INFO  ProgressMeter - Traversal complete. Processed 145 total regions in 1.3 minutes.\n",
      "14:20:21.157 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 0.13169395\n",
      "14:20:21.157 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 44.884707117000005\n",
      "14:20:21.157 INFO  SmithWatermanAligner - Total compute time in java Smith-Waterman : 4.13 sec\n",
      "14:20:21.157 INFO  HaplotypeCaller - Shutting down engine\n",
      "[March 24, 2021 2:20:21 PM EDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 1.30 minutes.\n",
      "Runtime.totalMemory()=981467136\n"
     ]
    }
   ],
   "source": [
    "module load gatk/4.1.9.0\n",
    "\n",
    "java -jar $GATK_JAR HaplotypeCaller \\\n",
    "          -R data/genome/NC_045512.2.fa \\\n",
    "          -I result/dedup_reads.bam \\\n",
    "          -O result/raw_variants.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Split the variants to snps and indels and filter SNPs by quality/calling confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar defined in environment variable GATK_LOCAL_JAR\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R data/genome/NC_045512.2.fa -V result/raw_variants.vcf -select-type SNP -O result/raw_snps.vcf\n",
      "14:24:16.684 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "Mar 24, 2021 2:24:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine\n",
      "INFO: Failed to detect whether we are running on Google Compute Engine.\n",
      "14:24:16.800 INFO  SelectVariants - ------------------------------------------------------------\n",
      "14:24:16.800 INFO  SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0\n",
      "14:24:16.800 INFO  SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "14:24:16.800 INFO  SelectVariants - Executing as ycc520@cs053.nyu.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64\n",
      "14:24:16.801 INFO  SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_271-b09\n",
      "14:24:16.801 INFO  SelectVariants - Start Date/Time: March 24, 2021 2:24:16 PM EDT\n",
      "14:24:16.801 INFO  SelectVariants - ------------------------------------------------------------\n",
      "14:24:16.801 INFO  SelectVariants - ------------------------------------------------------------\n",
      "14:24:16.801 INFO  SelectVariants - HTSJDK Version: 2.23.0\n",
      "14:24:16.801 INFO  SelectVariants - Picard Version: 2.23.3\n",
      "14:24:16.801 INFO  SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "14:24:16.801 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "14:24:16.801 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n",
      "14:24:16.801 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "14:24:16.801 INFO  SelectVariants - Deflater: IntelDeflater\n",
      "14:24:16.801 INFO  SelectVariants - Inflater: IntelInflater\n",
      "14:24:16.801 INFO  SelectVariants - GCS max retries/reopens: 20\n",
      "14:24:16.801 INFO  SelectVariants - Requester pays: disabled\n",
      "14:24:16.801 INFO  SelectVariants - Initializing engine\n",
      "14:24:17.057 INFO  FeatureManager - Using codec VCFCodec to read file file:///scratch/ycc520/ag_recitation/asm_03_key/result/raw_variants.vcf\n",
      "14:24:17.078 INFO  SelectVariants - Done initializing engine\n",
      "14:24:17.098 INFO  ProgressMeter - Starting traversal\n",
      "14:24:17.098 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute\n",
      "14:24:17.114 INFO  ProgressMeter -             unmapped              0.0                    15          56250.0\n",
      "14:24:17.114 INFO  ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.\n",
      "14:24:17.116 INFO  SelectVariants - Shutting down engine\n",
      "[March 24, 2021 2:24:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.01 minutes.\n",
      "Runtime.totalMemory()=336592896\n",
      "Using GATK jar /share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar defined in environment variable GATK_LOCAL_JAR\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R data/genome/NC_045512.2.fa -V result/raw_variants.vcf -select-type INDEL -O result/raw_indels.vcf\n",
      "14:24:19.252 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "Mar 24, 2021 2:24:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine\n",
      "INFO: Failed to detect whether we are running on Google Compute Engine.\n",
      "14:24:19.365 INFO  SelectVariants - ------------------------------------------------------------\n",
      "14:24:19.365 INFO  SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0\n",
      "14:24:19.365 INFO  SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "14:24:19.366 INFO  SelectVariants - Executing as ycc520@cs053.nyu.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64\n",
      "14:24:19.366 INFO  SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_271-b09\n",
      "14:24:19.366 INFO  SelectVariants - Start Date/Time: March 24, 2021 2:24:19 PM EDT\n",
      "14:24:19.366 INFO  SelectVariants - ------------------------------------------------------------\n",
      "14:24:19.366 INFO  SelectVariants - ------------------------------------------------------------\n",
      "14:24:19.366 INFO  SelectVariants - HTSJDK Version: 2.23.0\n",
      "14:24:19.366 INFO  SelectVariants - Picard Version: 2.23.3\n",
      "14:24:19.366 INFO  SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "14:24:19.366 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "14:24:19.366 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n",
      "14:24:19.366 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "14:24:19.366 INFO  SelectVariants - Deflater: IntelDeflater\n",
      "14:24:19.366 INFO  SelectVariants - Inflater: IntelInflater\n",
      "14:24:19.367 INFO  SelectVariants - GCS max retries/reopens: 20\n",
      "14:24:19.367 INFO  SelectVariants - Requester pays: disabled\n",
      "14:24:19.367 INFO  SelectVariants - Initializing engine\n",
      "14:24:19.613 INFO  FeatureManager - Using codec VCFCodec to read file file:///scratch/ycc520/ag_recitation/asm_03_key/result/raw_variants.vcf\n",
      "14:24:19.627 INFO  SelectVariants - Done initializing engine\n",
      "14:24:19.647 INFO  ProgressMeter - Starting traversal\n",
      "14:24:19.647 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute\n",
      "14:24:19.659 INFO  ProgressMeter -             unmapped              0.0                     5          25000.0\n",
      "14:24:19.659 INFO  ProgressMeter - Traversal complete. Processed 5 total variants in 0.0 minutes.\n",
      "14:24:19.661 INFO  SelectVariants - Shutting down engine\n",
      "[March 24, 2021 2:24:19 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.01 minutes.\n",
      "Runtime.totalMemory()=319815680\n"
     ]
    }
   ],
   "source": [
    "gatk SelectVariants \\\n",
    "     -R data/genome/NC_045512.2.fa \\\n",
    "     -V result/raw_variants.vcf \\\n",
    "     -select-type SNP \\\n",
    "     -O result/raw_snps.vcf\n",
    "     \n",
    "gatk SelectVariants \\\n",
    "     -R data/genome/NC_045512.2.fa \\\n",
    "     -V result/raw_variants.vcf \\\n",
    "     -select-type INDEL \\\n",
    "     -O result/raw_indels.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:25:07.637 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "Mar 24, 2021 2:25:07 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine\n",
      "INFO: Failed to detect whether we are running on Google Compute Engine.\n",
      "14:25:07.757 INFO  VariantFiltration - ------------------------------------------------------------\n",
      "14:25:07.757 INFO  VariantFiltration - The Genome Analysis Toolkit (GATK) v4.1.9.0\n",
      "14:25:07.757 INFO  VariantFiltration - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "14:25:07.757 INFO  VariantFiltration - Executing as ycc520@cs053.nyu.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64\n",
      "14:25:07.757 INFO  VariantFiltration - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_271-b09\n",
      "14:25:07.757 INFO  VariantFiltration - Start Date/Time: March 24, 2021 2:25:07 PM EDT\n",
      "14:25:07.757 INFO  VariantFiltration - ------------------------------------------------------------\n",
      "14:25:07.757 INFO  VariantFiltration - ------------------------------------------------------------\n",
      "14:25:07.758 INFO  VariantFiltration - HTSJDK Version: 2.23.0\n",
      "14:25:07.758 INFO  VariantFiltration - Picard Version: 2.23.3\n",
      "14:25:07.758 INFO  VariantFiltration - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "14:25:07.758 INFO  VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "14:25:07.758 INFO  VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n",
      "14:25:07.758 INFO  VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "14:25:07.758 INFO  VariantFiltration - Deflater: IntelDeflater\n",
      "14:25:07.758 INFO  VariantFiltration - Inflater: IntelInflater\n",
      "14:25:07.758 INFO  VariantFiltration - GCS max retries/reopens: 20\n",
      "14:25:07.758 INFO  VariantFiltration - Requester pays: disabled\n",
      "14:25:07.758 INFO  VariantFiltration - Initializing engine\n",
      "14:25:08.015 INFO  FeatureManager - Using codec VCFCodec to read file file:///scratch/ycc520/ag_recitation/asm_03_key/result/raw_snps.vcf\n",
      "14:25:08.031 INFO  VariantFiltration - Done initializing engine\n",
      "14:25:08.083 INFO  ProgressMeter - Starting traversal\n",
      "14:25:08.083 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute\n",
      "14:25:08.101 INFO  ProgressMeter -             unmapped              0.0                    15          50000.0\n",
      "14:25:08.101 INFO  ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.\n",
      "14:25:08.103 INFO  VariantFiltration - Shutting down engine\n",
      "[March 24, 2021 2:25:08 PM EDT] org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration done. Elapsed time: 0.01 minutes.\n",
      "Runtime.totalMemory()=343932928\n"
     ]
    }
   ],
   "source": [
    "# Filter SNPs with quality measures\n",
    "\n",
    "java -jar $GATK_JAR VariantFiltration \\\n",
    "          -R data/genome/NC_045512.2.fa \\\n",
    "          -V result/raw_snps.vcf \\\n",
    "          --filter-name \"QD_filter\" \\\n",
    "          -filter \"QD<2.0\" \\\n",
    "          --filter-name \"FS_filter\" \\\n",
    "          -filter \"FS>60.0\" \\\n",
    "          --filter-name \"MQ_filter\" \\\n",
    "          -filter \"MQ<40.0\" \\\n",
    "          --filter-name \"SOR_filter\" \\\n",
    "          -filter \"SOR>10.0\" \\\n",
    "          -O result/filtered_snps.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Annotate the filtered SNPs with snpEff\n",
    "\n",
    "snpEff annotates snps and indels by incorporating the information from a gene model. This allows us to learn whether a mutation is within a gene or in the intergenic region, and if a mutations causes the amino acid sequence of a gene to change or terminate prematurely.\n",
    "\n",
    "snpEff has annotation database generated for common organisms but not for SARS-CoV-2. The database was thus manually curated and provided for the assignment. I will show how the annotation is generated below and annotate the filtered snp list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SARS-CoV-2 genome, version GCF_009858895.2\n",
      "sarscov2.genome : SARS_COV_2\n"
     ]
    }
   ],
   "source": [
    "# To add a custom annotation, first we need to give it a name in the config file.\n",
    "cat snpEff.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two files are downloaded and renamed as `sequences.fa` and `genes.gff` respectively in the folder name defined in `snpEff.config`.\n",
    "\n",
    "If the genome is defined as sars.cov2.genome, the two files should be stored in data/sarscov2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "├── genome\n",
      "│   ├── NC_045512.2.dict\n",
      "│   ├── NC_045512.2.fa\n",
      "│   ├── NC_045512.2.fa.amb\n",
      "│   ├── NC_045512.2.fa.ann\n",
      "│   ├── NC_045512.2.fa.bwt\n",
      "│   ├── NC_045512.2.fa.fai\n",
      "│   ├── NC_045512.2.fa.pac\n",
      "│   └── NC_045512.2.fa.sa\n",
      "├── raw_seq\n",
      "│   ├── SRR11801823_1.fastq\n",
      "│   └── SRR11801823_2.fastq\n",
      "└── sarscov2\n",
      "    ├── genes.gff\n",
      "    └── sequences.fa\n",
      "\n",
      "3 directories, 12 files\n"
     ]
    }
   ],
   "source": [
    "tree data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00\tSnpEff version SnpEff 4.3t (build 2017-11-24 10:18), by Pablo Cingolani\n",
      "00:00:00\tCommand: 'build'\n",
      "00:00:00\tBuilding database for 'sarscov2'\n",
      "00:00:00\tReading configuration file 'snpEff.config'. Genome: 'sarscov2'\n",
      "00:00:00\tReading config file: /scratch/ycc520/ag_recitation/asm_03_key/snpEff.config\n",
      "00:00:00\tdone\n",
      "Reading GFF3 data file  : '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/genes.gff'\n",
      "WARNING: Cannot find transcript 'TRANSCRIPT_id-NC_045512.2:1..265'. File '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/genes.gff' line 9\t'NC_045512.2\tRefSeq\tfive_prime_UTR\t1\t265\t.\t+\t.\tID=id-NC_045512.2:1..265;gbkey=5'UTR'\n",
      "WARNING: Could not add UTR. File '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/genes.gff' line 9\t'NC_045512.2\tRefSeq\tfive_prime_UTR\t1\t265\t.\t+\t.\tID=id-NC_045512.2:1..265;gbkey=5'UTR'\n",
      "WARNING: Cannot find transcript 'TRANSCRIPT_id-NC_045512.2:29675..29903'. File '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/genes.gff' line 65\t'NC_045512.2\tRefSeq\tthree_prime_UTR\t29675\t29903\t.\t+\t.\tID=id-NC_045512.2:29675..29903;gbkey=3'UTR'\n",
      "WARNING: Could not add UTR. File '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/genes.gff' line 65\t'NC_045512.2\tRefSeq\tthree_prime_UTR\t29675\t29903\t.\t+\t.\tID=id-NC_045512.2:29675..29903;gbkey=3'UTR'\n",
      "\n",
      "\tTotal: 11 markers added.\n",
      "\n",
      "\tCreate exons from CDS (if needed): .............\n",
      "\tExons created for 11 transcripts.\n",
      "\n",
      "\tDeleting redundant exons (if needed): .\n",
      "\t\t0\t\n",
      "\t\tTotal transcripts with deleted exons: 1\n",
      "\n",
      "\tCollapsing zero length introns (if needed): .\n",
      "\t\t0\t\n",
      "\t\tTotal collapsed transcripts: 1\n",
      "\tReading sequences   :\n",
      "\tFASTA file: '/scratch/ycc520/ag_recitation/asm_03_key/./data/genomes/sarscov2.fa' not found.\n",
      "\tReading FASTA file: '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/sequences.fa'\n",
      "\t\tReading sequence 'NC_045512.2', length: 29903\n",
      "\t\tAdding genomic sequences to exons: \tDone (11 sequences added, 0 ignored).\n",
      "\tTotal: 11 sequences added, 0 sequences ignored.\n",
      "\n",
      "\tAdjusting transcripts: \n",
      "\tAdjusting genes: \n",
      "\tAdjusting chromosomes lengths: \n",
      "\tRanking exons: \n",
      "\tCreate UTRs from CDS (if needed): \n",
      "\tCorrecting exons based on frame information.\n",
      "\tWARNING: All frames are zero! This seems rather odd, please check that 'frame' information in your 'genes' file is accurate.\n",
      "\n",
      "\tRemove empty chromosomes: \n",
      "\n",
      "\tMarking as 'coding' from CDS information: \n",
      "\tDone: 0 transcripts marked\n",
      "#-----------------------------------------------\n",
      "# Genome name                : 'SARS_COV_2'\n",
      "# Genome version             : 'sarscov2'\n",
      "# Genome ID                  : 'sarscov2[0]'\n",
      "# Has protein coding info    : true\n",
      "# Has Tr. Support Level info : true\n",
      "# Genes                      : 11\n",
      "# Protein coding genes       : 11\n",
      "#-----------------------------------------------\n",
      "# Transcripts                : 11\n",
      "# Avg. transcripts per gene  : 1.00\n",
      "# TSL transcripts            : 0\n",
      "#-----------------------------------------------\n",
      "# Checked transcripts        : \n",
      "#               AA sequences :      0 ( 0.00% )\n",
      "#              DNA sequences :      0 ( 0.00% )\n",
      "#-----------------------------------------------\n",
      "# Protein coding transcripts : 11\n",
      "#              Length errors :      1 ( 9.09% )\n",
      "#  STOP codons in CDS errors :      1 ( 9.09% )\n",
      "#         START codon errors :      0 ( 0.00% )\n",
      "#        STOP codon warnings :      0 ( 0.00% )\n",
      "#              UTR sequences :      0 ( 0.00% )\n",
      "#               Total Errors :      1 ( 9.09% )\n",
      "# WARNING                    : No protein coding transcript has UTR\n",
      "#-----------------------------------------------\n",
      "# Cds                        : 11\n",
      "# Exons                      : 11\n",
      "# Exons with sequence        : 11\n",
      "# Exons without sequence     : 0\n",
      "# Avg. exons per transcript  : 1.00\n",
      "#-----------------------------------------------\n",
      "# Number of chromosomes      : 1\n",
      "# Chromosomes                : Format 'chromo_name size codon_table'\n",
      "#\t\t'NC_045512.2'\t29903\tStandard\n",
      "#-----------------------------------------------\n",
      "\n",
      "00:00:00\tCaracterizing exons by splicing (stage 1) : \n",
      "\t\n",
      "00:00:00\tCaracterizing exons by splicing (stage 2) : \n",
      "\t00:00:00\tdone.\n",
      "00:00:00\t[Optional] Rare amino acid annotations\n",
      "00:00:00\tWarning: Cannot read optional protein sequence file '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/protein.fa', nothing done.\n",
      "00:00:00\tSaving database\n",
      "00:00:00\t[Optional] Reading regulation elements: GFF\n",
      "00:00:00\tWarning: Cannot read optional regulation file '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/regulation.gff', nothing done.\n",
      "00:00:00\t[Optional] Reading regulation elements: BED \n",
      "00:00:00\tCannot find optional regulation dir '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/regulation.bed/', nothing done.\n",
      "00:00:00\t[Optional] Reading motifs: GFF\n",
      "00:00:00\tWarning: Cannot open PWMs file /scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/pwms.bin. Nothing done\n",
      "00:00:00\tDone\n",
      "00:00:00\tLogging\n",
      "00:00:01\tChecking for updates...\n",
      "00:00:01\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Then, the annotation database is generated from the refernce genome (fasta) and gene model (gff/gtf)\n",
    "module load snpeff/4.3t\n",
    "\n",
    "# -v points to where you put the .fa and .gff files under data/, in our case, it's sarscov2.\n",
    "java -jar $SNPEFF_JAR build -gff3 -v sarscov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00\tSnpEff version SnpEff 4.3t (build 2017-11-24 10:18), by Pablo Cingolani\n",
      "00:00:00\tCommand: 'ann'\n",
      "00:00:00\tReading configuration file 'snpEff.config'. Genome: 'sarscov2'\n",
      "00:00:00\tReading config file: /scratch/ycc520/ag_recitation/asm_03_key/snpEff.config\n",
      "00:00:00\tdone\n",
      "00:00:00\tReading database for genome version 'sarscov2' from file '/scratch/ycc520/ag_recitation/asm_03_key/./data/sarscov2/snpEffectPredictor.bin' (this might take a while)\n",
      "00:00:00\tdone\n",
      "00:00:00\tLoading Motifs and PWMs\n",
      "00:00:00\tBuilding interval forest\n",
      "00:00:00\tdone.\n",
      "00:00:00\tGenome stats :\n",
      "#-----------------------------------------------\n",
      "# Genome name                : 'SARS_COV_2'\n",
      "# Genome version             : 'sarscov2'\n",
      "# Genome ID                  : 'sarscov2[0]'\n",
      "# Has protein coding info    : true\n",
      "# Has Tr. Support Level info : true\n",
      "# Genes                      : 11\n",
      "# Protein coding genes       : 11\n",
      "#-----------------------------------------------\n",
      "# Transcripts                : 11\n",
      "# Avg. transcripts per gene  : 1.00\n",
      "# TSL transcripts            : 0\n",
      "#-----------------------------------------------\n",
      "# Checked transcripts        : \n",
      "#               AA sequences :      0 ( 0.00% )\n",
      "#              DNA sequences :      0 ( 0.00% )\n",
      "#-----------------------------------------------\n",
      "# Protein coding transcripts : 11\n",
      "#              Length errors :      1 ( 9.09% )\n",
      "#  STOP codons in CDS errors :      1 ( 9.09% )\n",
      "#         START codon errors :      0 ( 0.00% )\n",
      "#        STOP codon warnings :      0 ( 0.00% )\n",
      "#              UTR sequences :      0 ( 0.00% )\n",
      "#               Total Errors :      1 ( 9.09% )\n",
      "# WARNING                    : No protein coding transcript has UTR\n",
      "#-----------------------------------------------\n",
      "# Cds                        : 11\n",
      "# Exons                      : 11\n",
      "# Exons with sequence        : 11\n",
      "# Exons without sequence     : 0\n",
      "# Avg. exons per transcript  : 1.00\n",
      "#-----------------------------------------------\n",
      "# Number of chromosomes      : 1\n",
      "# Chromosomes                : Format 'chromo_name size codon_table'\n",
      "#\t\t'NC_045512.2'\t29903\tStandard\n",
      "#-----------------------------------------------\n",
      "\n",
      "00:00:00\tPredicting variants\n",
      "\n",
      "WARNINGS: Some warning were detected\n",
      "Warning type\tNumber of warnings\n",
      "WARNING_TRANSCRIPT_MULTIPLE_STOP_CODONS\t12\n",
      "\n",
      "\n",
      "00:00:00\tCreating summary file: snpEff_summary.html\n",
      "00:00:00\tCreating genes file: snpEff_genes.txt\n",
      "00:00:00\tdone.\n",
      "00:00:00\tLogging\n",
      "00:00:01\tChecking for updates...\n",
      "00:00:01\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Now we can annotate the snps with the newly generated database\n",
    "\n",
    "java -jar $SNPEFF_JAR \\\n",
    "     -v sarscov2 \\\n",
    "     result/filtered_snps.vcf > result/filtered_snps.ann.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: (15%) Which gene has the most filtered SNPs? (Hint: snpEff provides a gene summary table)\n",
    "\n",
    "You can open the `snpEff_genes.txt` with Excel or R for better readability. It is worth noting that `variant_impact` and `variant_effect_*` could overlap: A missense variant could have moderate impact, so if you simply take the row mean, it will be counted twice.\n",
    "\n",
    "Personally, I would count only the columns for `variant_effect`: They account for mutations upstream and down stream of a gene, and mutations within a gene (synonymous or missense).\n",
    "\n",
    "You can also visualize the SNPs in IGV and count manually, but this will not be possible for a profiling of larger scale where you might have hundreds of SNPs in a gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The following table is formatted as tab separated values. \n",
      "#GeneName\tGeneId\tTranscriptId\tBioType\tvariants_impact_LOW\tvariants_impact_MODERATE\tvariants_impact_MODIFIER\tvariants_effect_downstream_gene_variant\tvariants_effect_missense_variant\tvariants_effect_synonymous_variant\tvariants_effect_upstream_gene_variant\n",
      "E\tgene-GU280_gp04\tTRANSCRIPT_gene-GU280_gp04\tprotein_coding\t0\t0\t4\t3\t0\t0\t1\n",
      "M\tgene-GU280_gp05\tTRANSCRIPT_gene-GU280_gp05\tprotein_coding\t0\t0\t4\t3\t0\t0\t1\n",
      "N\tgene-GU280_gp10\tTRANSCRIPT_gene-GU280_gp10\tprotein_coding\t1\t2\t1\t0\t2\t1\t1\n",
      "ORF10\tgene-GU280_gp11\tTRANSCRIPT_gene-GU280_gp11\tprotein_coding\t0\t0\t3\t0\t0\t0\t3\n",
      "ORF1ab\tgene-GU280_gp01\tTRANSCRIPT_gene-GU280_gp01\tprotein_coding\t3\t7\t2\t1\t7\t3\t1\n",
      "ORF3a\tgene-GU280_gp03\tTRANSCRIPT_gene-GU280_gp03\tprotein_coding\t0\t0\t4\t3\t0\t0\t1\n",
      "ORF6\tgene-GU280_gp06\tTRANSCRIPT_gene-GU280_gp06\tprotein_coding\t0\t0\t4\t3\t0\t0\t1\n",
      "ORF7a\tgene-GU280_gp07\tTRANSCRIPT_gene-GU280_gp07\tprotein_coding\t0\t0\t4\t3\t0\t0\t1\n"
     ]
    }
   ],
   "source": [
    "head snpEff_genes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: (15%) Is there any missense mutation on the spike protein (gene S)? If there is, what is the amino acid change? Please answer in the format of original AA - position - mutated AA (For example, the change of the 93th amino acid from Glycine to Alanine is G93A).\n",
    "\n",
    "`vcf_summary.sh` helps you to format the VCF file so you can see the mutations and there corresponding genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  missense_variant         MODERATE  ORF1ab            p.Ile300Phe\n",
      "G  missense_variant         MODERATE  ORF1ab            p.Ser1218Gly\n",
      "C  missense_variant         MODERATE  ORF1ab            p.Ile2057Thr\n",
      "T  missense_variant         MODERATE  ORF1ab            p.Gln2058His\n",
      "A  missense_variant         MODERATE  ORF1ab            p.Met4766Ile\n",
      "T  missense_variant         MODERATE  ORF1ab            p.Ser5585Ile\n",
      "C  missense_variant         MODERATE  ORF1ab            p.Leu5624Pro\n",
      "G  missense_variant         MODERATE  S                 p.Asp614Gly\n",
      "A  missense_variant         MODERATE  N                 p.Arg203Lys\n",
      "C  missense_variant         MODERATE  N                 p.Gly204Arg\n"
     ]
    }
   ],
   "source": [
    "./vcf_summary.sh result/filtered_snps.ann.vcf | grep miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can clearly see that D614G mutation is present in the spike protein. If you search for this mutation, you will find that this mutation soon become very prevalent globally because it is associated with higher transmissibility, presumably by reducing spike protein shedding and enhanced incorporation of spike proteins to the virion [[1]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7310631/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
